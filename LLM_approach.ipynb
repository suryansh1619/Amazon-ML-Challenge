{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"name":"notebook904f98a855"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9407881,"sourceType":"datasetVersion","datasetId":5699304}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install qwen-vl-utils","metadata":{"id":"aNO5ULHbqsvT","execution":{"iopub.status.busy":"2024-09-15T22:07:22.471525Z","iopub.execute_input":"2024-09-15T22:07:22.47221Z","iopub.status.idle":"2024-09-15T22:07:36.351705Z","shell.execute_reply.started":"2024-09-15T22:07:22.472165Z","shell.execute_reply":"2024-09-15T22:07:36.35054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers","metadata":{"id":"VPkKZbCBqsvT","execution":{"iopub.status.busy":"2024-09-15T22:07:36.354075Z","iopub.execute_input":"2024-09-15T22:07:36.35445Z","iopub.status.idle":"2024-09-15T22:08:28.517092Z","shell.execute_reply.started":"2024-09-15T22:07:36.354413Z","shell.execute_reply":"2024-09-15T22:08:28.515992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install accelerate transformers pillow","metadata":{"execution":{"iopub.status.busy":"2024-09-15T22:08:28.518491Z","iopub.execute_input":"2024-09-15T22:08:28.518818Z","iopub.status.idle":"2024-09-15T22:08:41.389855Z","shell.execute_reply.started":"2024-09-15T22:08:28.518784Z","shell.execute_reply":"2024-09-15T22:08:41.388686Z"},"id":"oo_5WxjQw3Yy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from accelerate import Accelerator\nfrom accelerate.utils import gather_object\n\naccelerator = Accelerator()\n\n# each GPU creates a string\nmessage=[ f\"Hello this is GPU {accelerator.process_index}\" ]\n\n# collect the messages from all GPUs\nmessages=gather_object(message)\n\n# output the messages only on the main process with accelerator.print()\naccelerator.print(messages)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T22:08:41.392287Z","iopub.execute_input":"2024-09-15T22:08:41.392604Z","iopub.status.idle":"2024-09-15T22:08:43.686854Z","shell.execute_reply.started":"2024-09-15T22:08:41.39257Z","shell.execute_reply":"2024-09-15T22:08:43.685908Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"HhhMyrnzw3Yz","outputId":"d8483c64-1f9e-4a54-c3f6-1d4b7713e80f","trusted":true},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"['Hello this is GPU 0']\n"}]},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom PIL import Image\nimport gc\nimport time\nfrom accelerate import Accelerator\nfrom accelerate.utils import gather_object\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T22:09:40.70769Z","iopub.execute_input":"2024-09-15T22:09:40.708558Z","iopub.status.idle":"2024-09-15T22:09:40.713453Z","shell.execute_reply.started":"2024-09-15T22:09:40.708516Z","shell.execute_reply":"2024-09-15T22:09:40.712509Z"},"id":"t6CIZ41Mw3Yz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n# from qwen_vl_utils import process_vision_info\n# import torch\n# import os\n# # os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments'\n\n# # model = Qwen2VLForConditionalGeneration.from_pretrained(\n# #     \"/mnt/Main Drive/Codes/Deep Learning/LLM/Qwen2-VL-2B-Instruct\",\n# #     torch_dtype=torch.bfloat16,\n# #     device_map='auto'\n# # )\n\n# model = Qwen2VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2-VL-2B-Instruct\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"sdpa\",\n#     device_map=\"auto\",\n# )\n# min_pixels = 256 * 28 * 28\n# max_pixels = 1280 * 28 * 28\n# processor = AutoProcessor.from_pretrained(\n#     \"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n# )","metadata":{"id":"H71YVNZtqsvU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image","metadata":{"id":"GWsfUG4RqsvU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"id":"MYxcdYJP1taM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/null-set/null/voltage_null.csv\")\ndata= data.iloc[1676:2676]","metadata":{"execution":{"iopub.status.busy":"2024-09-15T22:09:55.672927Z","iopub.execute_input":"2024-09-15T22:09:55.673821Z","iopub.status.idle":"2024-09-15T22:09:55.715893Z","shell.execute_reply.started":"2024-09-15T22:09:55.673779Z","shell.execute_reply":"2024-09-15T22:09:55.715135Z"},"id":"QYlqSErJw3Y0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_filename(url):\n    return os.path.join('/kaggle/input/null-set/test_null/test_null', os.path.basename(url))","metadata":{"id":"5fYDKAdLlK_L","execution":{"iopub.status.busy":"2024-09-15T22:09:56.475473Z","iopub.execute_input":"2024-09-15T22:09:56.475844Z","iopub.status.idle":"2024-09-15T22:09:56.481942Z","shell.execute_reply.started":"2024-09-15T22:09:56.475809Z","shell.execute_reply":"2024-09-15T22:09:56.481151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['image_path'] = data['image_link'].apply(extract_filename)","metadata":{"id":"01ZJpNWjqsvV","execution":{"iopub.status.busy":"2024-09-15T22:09:57.048947Z","iopub.execute_input":"2024-09-15T22:09:57.049338Z","iopub.status.idle":"2024-09-15T22:09:57.069975Z","shell.execute_reply.started":"2024-09-15T22:09:57.049301Z","shell.execute_reply":"2024-09-15T22:09:57.069093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"id":"n19s7SH114ys"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up Accelerator for multi-GPU\naccelerator = Accelerator()","metadata":{"id":"oOjr1MpHz5DX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_batch(batch_data, model, processor):\n    results = []\n    for item in batch_data:\n        index, image_path, entity_name = item['index'], item['image_path'], item['entity_name']\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image\",\n                        \"image\": Image.open(image_path),\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": f\"what is the voltage given in the image and give the answer in 3 words and in Volt as in the image\",\n                        \"resized_height\": 720,\n                    }\n                ]\n            }\n        ]\n\n        text = processor.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        image_inputs, video_inputs = process_vision_info(messages)\n        inputs = processor(\n            text=[text],\n            images=image_inputs,\n            videos=video_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        inputs = inputs.to(accelerator.device)\n\n        with torch.no_grad():\n            generated_ids = model.generate(**inputs, max_new_tokens=512)\n\n        generated_ids_trimmed = [\n            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n        ]\n        output_text = processor.batch_decode(\n            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )\n        # print(output_text)\n        results.append({\n            \"index\": index,\n            \"image_path\": image_path,\n            \"entity_name\": entity_name,\n            \"textual_data\": output_text[0]\n        })\n        # print(results)\n\n    return results\n","metadata":{"id":"zC0G0KSWz3FU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    batch_size = 10  # Adjust based on your GPU memory\n    output_file = 'Voltage_null_output.csv'\n\n    # Initialize the output CSV file on the main process\n    if accelerator.is_main_process:\n        if not os.path.exists(output_file):\n            pd.DataFrame(columns=['index', 'image_path', 'entity_name', 'textual_data']).to_csv(output_file, index=False)\n\n    # Load your data\n#     data = pd.read_csv('/kaggle/input/your_dataset/your_data.csv')  # Adjust path as needed\n\n    # Load model and processor\n    model_path = \"Qwen/Qwen2-VL-2B-Instruct\"  # Adjust path as needed\n    model = Qwen2VLForConditionalGeneration.from_pretrained(\n        model_path,\n        torch_dtype=torch.bfloat16,\n#         attn_implementation=\"flash_attention_2\",\n        device_map=\"auto\",  # This will distribute the model across available GPUs\n    )\n\n    min_pixels = 256 * 28 * 28\n    max_pixels = 1280 * 28 * 28\n    processor = AutoProcessor.from_pretrained(\n        model_path, min_pixels=min_pixels, max_pixels=max_pixels\n    )\n\n    # Prepare model and processor for distributed setup\n    model, processor = accelerator.prepare(model, processor)\n\n    # Sync GPUs and start the timer\n    accelerator.wait_for_everyone()\n    start = time.time()\n\n    # Divide the data among available GPUs\n    with accelerator.split_between_processes(data.to_dict('records')) as local_data:\n        all_results = []\n\n        for i in range(0, len(local_data),batch_size):\n            batch = local_data[i:i+batch_size]\n            batch_results = process_batch(batch, model, processor)\n            all_results.extend(batch_results)\n\n            # Save intermediate results\n            if accelerator.is_main_process:\n                pd.DataFrame(batch_results).to_csv(output_file, mode='a', header=False, index=False)\n\n        all_results = [all_results]  # Wrap in list for gather_object\n\n    # Collect results from all GPUs\n    results_gathered = gather_object(all_results)\n\n    if accelerator.is_main_process:\n        # Combine and save final results\n        final_results = [item for sublist in results_gathered for item in sublist]\n        pd.DataFrame(final_results).to_csv(output_file, index=False)\n\n        time_diff = time.time() - start\n        print(f\"Total time elapsed: {time_diff:.2f} seconds\")\n        print(f\"Processed {len(final_results)} items\")\n\n    # Clear GPU memory\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"id":"Av-qd5Lx0Brs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T22:13:18.098591Z","iopub.execute_input":"2024-09-15T22:13:18.099131Z"},"id":"IiqXIx1Ew3Y1","trusted":true},"execution_count":null,"outputs":[]}]}